{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "- RDD API docs: http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD\n",
    "\n",
    "Rules of thumb:\n",
    "- Hit tab to auto-complete\n",
    "- To see all available methods, place a dot (.) after the RDD (e.g. words.) and hit tab \n",
    "- Use `.collect()` to see the contents of the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dunyangan/learning/data-eng-bootcamp/.venv_data_eng_bootcamp/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "# verify that you're using the virtual environment\n",
    "!which python\n",
    "\n",
    "# you should see: /path/to/data-eng-bootcamp/.venv_data_eng_bootcamp/bin/python\n",
    "# otherwise, stop the pyspark process in your terminal, activate the virtual environment, and run this command again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://duns-mbp:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# like in the pyspark shell, SparkContext is already defined\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RDD Transformations and Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Working with in-memory data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Working with numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16, 25]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda n: n ** 2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: double each number in the original rdd\n",
    "rdd.map(lambda n: n*2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: calculate sum of all numbers in the original rdd\n",
    "rdd.reduce(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: (i) square each number, (ii) filter out odd numbers (i.e. keep only even numbers) and (iii) calculate sum of remaining numbers (answer: 20)\n",
    "rdd.map(lambda x: x **2).filter(lambda x: x % 2 == 0).reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Working with strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sc.parallelize(['hello', 'world', 'goodbye', 'world'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world', 'goodbye', 'world']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts = words.map(lambda word: (word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 1), ('world', 1), ('goodbye', 1), ('world', 1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Given the list in the preceeding cell, how would you create a list of (word, count)?\n",
    "# hint: http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey\n",
    "wordcounts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 1), ('world', 2), ('goodbye', 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcounts.reduceByKey(lambda a,b: a+b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sc.parallelize([\"Basics of the Unix Philosophy\", \"The ‘Unix philosophy’ originated with Ken Thompson's early meditations on how to design a small but capable operating system with a clean service interface. It grew as the Unix culture learned things about how to get maximum leverage out of Thompson's design. It absorbed lessons from many sources along the way\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Basics of the Unix Philosophy',\n",
       " \"The ‘Unix philosophy’ originated with Ken Thompson's early meditations on how to design a small but capable operating system with a clean service interface. It grew as the Unix culture learned things about how to get maximum leverage out of Thompson's design. It absorbed lessons from many sources along the way\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Basics', 1),\n",
       " ('of', 2),\n",
       " ('the', 3),\n",
       " ('Unix', 2),\n",
       " ('Philosophy', 1),\n",
       " ('The', 1),\n",
       " ('‘Unix', 1),\n",
       " ('philosophy’', 1),\n",
       " ('originated', 1),\n",
       " ('with', 2),\n",
       " ('Ken', 1),\n",
       " (\"Thompson's\", 2),\n",
       " ('early', 1),\n",
       " ('meditations', 1),\n",
       " ('on', 1),\n",
       " ('how', 2),\n",
       " ('to', 2),\n",
       " ('design', 1),\n",
       " ('a', 2),\n",
       " ('small', 1),\n",
       " ('but', 1),\n",
       " ('capable', 1),\n",
       " ('operating', 1),\n",
       " ('system', 1),\n",
       " ('clean', 1),\n",
       " ('service', 1),\n",
       " ('interface.', 1),\n",
       " ('It', 2),\n",
       " ('grew', 1),\n",
       " ('as', 1),\n",
       " ('culture', 1),\n",
       " ('learned', 1),\n",
       " ('things', 1),\n",
       " ('about', 1),\n",
       " ('get', 1),\n",
       " ('maximum', 1),\n",
       " ('leverage', 1),\n",
       " ('out', 1),\n",
       " ('design.', 1),\n",
       " ('absorbed', 1),\n",
       " ('lessons', 1),\n",
       " ('from', 1),\n",
       " ('many', 1),\n",
       " ('sources', 1),\n",
       " ('along', 1),\n",
       " ('way', 1)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: word count, again! \n",
    "# hint: Highlight the whitespace between this cell and the next cell to see the hint\n",
    "\n",
    "sentences.flatMap(lambda str : str.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda a,b: a + b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='white'>Hint: use flatMap!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Basics', 1),\n",
       " ('of', 2),\n",
       " ('the', 3),\n",
       " ('Unix', 3),\n",
       " ('Philosophy', 1),\n",
       " ('The', 1),\n",
       " ('philosophy', 1),\n",
       " ('originated', 1),\n",
       " ('with', 2),\n",
       " ('Ken', 1),\n",
       " ('Thompsons', 2),\n",
       " ('early', 1),\n",
       " ('meditations', 1),\n",
       " ('on', 1),\n",
       " ('how', 2),\n",
       " ('to', 2),\n",
       " ('design', 2),\n",
       " ('a', 2),\n",
       " ('small', 1),\n",
       " ('but', 1),\n",
       " ('capable', 1),\n",
       " ('operating', 1),\n",
       " ('system', 1),\n",
       " ('clean', 1),\n",
       " ('service', 1),\n",
       " ('interface', 1),\n",
       " ('It', 2),\n",
       " ('grew', 1),\n",
       " ('as', 1),\n",
       " ('culture', 1),\n",
       " ('learned', 1),\n",
       " ('things', 1),\n",
       " ('about', 1),\n",
       " ('get', 1),\n",
       " ('maximum', 1),\n",
       " ('leverage', 1),\n",
       " ('out', 1),\n",
       " ('absorbed', 1),\n",
       " ('lessons', 1),\n",
       " ('from', 1),\n",
       " ('many', 1),\n",
       " ('sources', 1),\n",
       " ('along', 1),\n",
       " ('way', 1)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bonus task 1: strip non-alphabetical characters (e.g. ‘Unix and Unix)\n",
    "import re;\n",
    "sentences.flatMap(lambda str : str.split(\" \")).map(lambda s: re.sub('[^a-zA-Z]+', '', s)).map(lambda w: (w, 1)).reduceByKey(lambda a,b: a+b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 3),\n",
       " ('Unix', 3),\n",
       " ('of', 2),\n",
       " ('with', 2),\n",
       " ('Thompsons', 2),\n",
       " ('how', 2),\n",
       " ('to', 2),\n",
       " ('design', 2),\n",
       " ('a', 2),\n",
       " ('It', 2),\n",
       " ('Basics', 1),\n",
       " ('Philosophy', 1),\n",
       " ('The', 1),\n",
       " ('philosophy', 1),\n",
       " ('originated', 1),\n",
       " ('Ken', 1),\n",
       " ('early', 1),\n",
       " ('meditations', 1),\n",
       " ('on', 1),\n",
       " ('small', 1),\n",
       " ('but', 1),\n",
       " ('capable', 1),\n",
       " ('operating', 1),\n",
       " ('system', 1),\n",
       " ('clean', 1),\n",
       " ('service', 1),\n",
       " ('interface', 1),\n",
       " ('grew', 1),\n",
       " ('as', 1),\n",
       " ('culture', 1),\n",
       " ('learned', 1),\n",
       " ('things', 1),\n",
       " ('about', 1),\n",
       " ('get', 1),\n",
       " ('maximum', 1),\n",
       " ('leverage', 1),\n",
       " ('out', 1),\n",
       " ('absorbed', 1),\n",
       " ('lessons', 1),\n",
       " ('from', 1),\n",
       " ('many', 1),\n",
       " ('sources', 1),\n",
       " ('along', 1),\n",
       " ('way', 1)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bonus task 2: sort word count by frequency in descending order\n",
    "# hint: http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sortBy\n",
    "sentences.flatMap(lambda str : str.split(\" \")).map(lambda s: re.sub('[^a-zA-Z]+', '', s)).map(lambda w: (w, 1)).reduceByKey(lambda a,b: a+b).sortBy(lambda x : x[1],ascending=False).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Creating a RDD by reading from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Word count on a text file!\n",
    "# Print the top 15 most frequent (word, count) pairs to the screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sc.textFile('../data/word_count/unix-philosophy-basics.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 214),\n",
       " ('to', 185),\n",
       " ('of', 161),\n",
       " ('and', 133),\n",
       " ('is', 109),\n",
       " ('a', 99),\n",
       " ('in', 79),\n",
       " ('it', 65),\n",
       " ('that', 64),\n",
       " ('be', 58),\n",
       " ('for', 53),\n",
       " ('you', 50),\n",
       " ('Rule', 46),\n",
       " ('programs', 43),\n",
       " ('as', 41),\n",
       " ('Unix', 38),\n",
       " ('with', 36),\n",
       " ('code', 34),\n",
       " ('are', 32),\n",
       " ('can', 29)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.flatMap(lambda str : str.split(\" \")).map(lambda s: re.sub('[^a-zA-Z]+', '', s)).filter(lambda s: s).map(lambda w: (w, 1)).reduceByKey(lambda a,b: a+b).sortBy(lambda x : x[1],ascending=False).take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: submit the preceeding task as a spark job\n",
    "# 1. Create a python file named ./jobs/unix_philosophy_word_count.py\n",
    "# 2. define spark session object\n",
    "#   - from pyspark import SparkContext\n",
    "#   - sc = SparkContext(\"local\", \"Unix Word Count\")\n",
    "# 3. Copy your solution code into the file\n",
    "# 4. submit the job: ${SPARK_HOME}/bin/spark-submit --master local ./jobs/unix_philosophy_word_count.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most top 15 most common words in this file?\n",
    "\n",
    "Highlight the section below to see the answer! vvv\n",
    "\n",
    "<font color='white'>\n",
    "[('', 226),\n",
    " ('the', 214),\n",
    " ('to', 185),\n",
    " ('of', 161),\n",
    " ('and', 133),\n",
    " ('is', 109),\n",
    " ('a', 99),\n",
    " ('in', 79),\n",
    " ('it', 65),\n",
    " ('that', 64),\n",
    " ('be', 58),\n",
    " ('for', 53),\n",
    " ('you', 50),\n",
    " ('Rule', 46),\n",
    " ('programs', 43)]\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dunyangan/learning/data-eng-bootcamp/.venv_data_eng_bootcamp/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_data_eng_bootcamp",
   "language": "python",
   "name": ".venv_data_eng_bootcamp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
